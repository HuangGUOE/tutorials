
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "advanced/neural_style_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_advanced_neural_style_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_neural_style_tutorial.py:


Neural Transfer Using PyTorch
=============================


**Author**: `Alexis Jacq <https://alexis-jacq.github.io>`_
 
**Edited by**: `Winston Herring <https://github.com/winston6>`_

Introduction
------------

This tutorial explains how to implement the `Neural-Style algorithm <https://arxiv.org/abs/1508.06576>`__
developed by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge.
Neural-Style, or Neural-Transfer, allows you to take an image and
reproduce it with a new artistic style. The algorithm takes three images,
an input image, a content-image, and a style-image, and changes the input 
to resemble the content of the content-image and the artistic style of the style-image.

 
.. figure:: /_static/img/neural-style/neuralstyle.png
   :alt: content1

.. GENERATED FROM PYTHON SOURCE LINES 26-49

Underlying Principle
--------------------

The principle is simple: we define two distances, one for the content
(:math:`D_C`) and one for the style (:math:`D_S`). :math:`D_C` measures how different the content
is between two images while :math:`D_S` measures how different the style is
between two images. Then, we take a third image, the input, and
transform it to minimize both its content-distance with the
content-image and its style-distance with the style-image. Now we can
import the necessary packages and begin the neural transfer.

Importing Packages and Selecting a Device
-----------------------------------------
Below is a  list of the packages needed to implement the neural transfer.

-  ``torch``, ``torch.nn``, ``numpy`` (indispensables packages for
   neural networks with PyTorch)
-  ``torch.optim`` (efficient gradient descents)
-  ``PIL``, ``PIL.Image``, ``matplotlib.pyplot`` (load and display
   images)
-  ``torchvision.transforms`` (transform PIL images into tensors)
-  ``torchvision.models`` (train or load pre-trained models)
-  ``copy`` (to deep copy the models; system package)

.. GENERATED FROM PYTHON SOURCE LINES 49-66

.. code-block:: default


    from __future__ import print_function

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim

    from PIL import Image
    import matplotlib.pyplot as plt

    import torchvision.transforms as transforms
    import torchvision.models as models

    import copy









.. GENERATED FROM PYTHON SOURCE LINES 67-73

Next, we need to choose which device to run the network on and import the
content and style images. Running the neural transfer algorithm on large
images takes longer and will go much faster when running on a GPU. We can
use ``torch.cuda.is_available()`` to detect if there is a GPU available.
Next, we set the ``torch.device`` for use throughout the tutorial. Also the ``.to(device)``
method is used to move tensors or modules to a desired device. 

.. GENERATED FROM PYTHON SOURCE LINES 73-76

.. code-block:: default


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")








.. GENERATED FROM PYTHON SOURCE LINES 77-97

Loading the Images
------------------

Now we will import the style and content images. The original PIL images have values between 0 and 255, but when
transformed into torch tensors, their values are converted to be between
0 and 1. The images also need to be resized to have the same dimensions.
An important detail to note is that neural networks from the
torch library are trained with tensor values ranging from 0 to 1. If you
try to feed the networks with 0 to 255 tensor images, then the activated
feature maps will be unable to sense the intended content and style.
However, pre-trained networks from the Caffe library are trained with 0
to 255 tensor images. 


.. Note::
    Here are links to download the images required to run the tutorial:
    `picasso.jpg <https://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg>`__ and
    `dancing.jpg <https://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg>`__.
    Download these two images and add them to a directory
    with name ``images`` in your current working directory.

.. GENERATED FROM PYTHON SOURCE LINES 97-120

.. code-block:: default


    # desired size of the output image
    imsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu

    loader = transforms.Compose([
        transforms.Resize(imsize),  # scale imported image
        transforms.ToTensor()])  # transform it into a torch tensor


    def image_loader(image_name):
        image = Image.open(image_name)
        # fake batch dimension required to fit network's input dimensions
        image = loader(image).unsqueeze(0)
        return image.to(device, torch.float)


    style_img = image_loader("./data/images/neural-style/picasso.jpg")
    content_img = image_loader("./data/images/neural-style/dancing.jpg")

    assert style_img.size() == content_img.size(), \
        "we need to import style and content images of the same size"









.. GENERATED FROM PYTHON SOURCE LINES 121-125

Now, let's create a function that displays an image by reconverting a 
copy of it to PIL format and displaying the copy using 
``plt.imshow``. We will try displaying the content and style images 
to ensure they were imported correctly.

.. GENERATED FROM PYTHON SOURCE LINES 125-146

.. code-block:: default


    unloader = transforms.ToPILImage()  # reconvert into PIL image

    plt.ion()

    def imshow(tensor, title=None):
        image = tensor.cpu().clone()  # we clone the tensor to not do changes on it
        image = image.squeeze(0)      # remove the fake batch dimension
        image = unloader(image)
        plt.imshow(image)
        if title is not None:
            plt.title(title)
        plt.pause(0.001) # pause a bit so that plots are updated


    plt.figure()
    imshow(style_img, title='Style Image')

    plt.figure()
    imshow(content_img, title='Content Image')




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /advanced/images/sphx_glr_neural_style_tutorial_001.png
         :alt: Style Image
         :srcset: /advanced/images/sphx_glr_neural_style_tutorial_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /advanced/images/sphx_glr_neural_style_tutorial_002.png
         :alt: Content Image
         :srcset: /advanced/images/sphx_glr_neural_style_tutorial_002.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 147-171

Loss Functions
--------------
Content Loss
~~~~~~~~~~~~

The content loss is a function that represents a weighted version of the
content distance for an individual layer. The function takes the feature
maps :math:`F_{XL}` of a layer :math:`L` in a network processing input :math:`X` and returns the
weighted content distance :math:`w_{CL}.D_C^L(X,C)` between the image :math:`X` and the
content image :math:`C`. The feature maps of the content image(:math:`F_{CL}`) must be
known by the function in order to calculate the content distance. We
implement this function as a torch module with a constructor that takes
:math:`F_{CL}` as an input. The distance :math:`\|F_{XL} - F_{CL}\|^2` is the mean square error
between the two sets of feature maps, and can be computed using ``nn.MSELoss``.

We will add this content loss module directly after the convolution
layer(s) that are being used to compute the content distance. This way
each time the network is fed an input image the content losses will be
computed at the desired layers and because of auto grad, all the
gradients will be computed. Now, in order to make the content loss layer
transparent we must define a ``forward`` method that computes the content
loss and then returns the layer’s input. The computed loss is saved as a
parameter of the module.


.. GENERATED FROM PYTHON SOURCE LINES 171-186

.. code-block:: default


    class ContentLoss(nn.Module):

        def __init__(self, target,):
            super(ContentLoss, self).__init__()
            # we 'detach' the target content from the tree used
            # to dynamically compute the gradient: this is a stated value,
            # not a variable. Otherwise the forward method of the criterion
            # will throw an error.
            self.target = target.detach()

        def forward(self, input):
            self.loss = F.mse_loss(input, self.target)
            return input








.. GENERATED FROM PYTHON SOURCE LINES 187-193

.. Note::
   **Important detail**: although this module is named ``ContentLoss``, it
   is not a true PyTorch Loss function. If you want to define your content
   loss as a PyTorch Loss function, you have to create a PyTorch autograd function 
   to recompute/implement the gradient manually in the ``backward``
   method.

.. GENERATED FROM PYTHON SOURCE LINES 195-217

Style Loss
~~~~~~~~~~

The style loss module is implemented similarly to the content loss
module. It will act as a transparent layer in a
network that computes the style loss of that layer. In order to
calculate the style loss, we need to compute the gram matrix :math:`G_{XL}`. A gram
matrix is the result of multiplying a given matrix by its transposed
matrix. In this application the given matrix is a reshaped version of
the feature maps :math:`F_{XL}` of a layer :math:`L`. :math:`F_{XL}` is reshaped to form :math:`\hat{F}_{XL}`, a :math:`K`\ x\ :math:`N`
matrix, where :math:`K` is the number of feature maps at layer :math:`L` and :math:`N` is the
length of any vectorized feature map :math:`F_{XL}^k`. For example, the first line
of :math:`\hat{F}_{XL}` corresponds to the first vectorized feature map :math:`F_{XL}^1`.

Finally, the gram matrix must be normalized by dividing each element by
the total number of elements in the matrix. This normalization is to
counteract the fact that :math:`\hat{F}_{XL}` matrices with a large :math:`N` dimension yield
larger values in the Gram matrix. These larger values will cause the
first layers (before pooling layers) to have a larger impact during the
gradient descent. Style features tend to be in the deeper layers of the
network so this normalization step is crucial.


.. GENERATED FROM PYTHON SOURCE LINES 217-232

.. code-block:: default


    def gram_matrix(input):
        a, b, c, d = input.size()  # a=batch size(=1)
        # b=number of feature maps
        # (c,d)=dimensions of a f. map (N=c*d)

        features = input.view(a * b, c * d)  # resise F_XL into \hat F_XL

        G = torch.mm(features, features.t())  # compute the gram product

        # we 'normalize' the values of the gram matrix
        # by dividing by the number of element in each feature maps.
        return G.div(a * b * c * d)









.. GENERATED FROM PYTHON SOURCE LINES 233-237

Now the style loss module looks almost exactly like the content loss
module. The style distance is also computed using the mean square
error between :math:`G_{XL}` and :math:`G_{SL}`.


.. GENERATED FROM PYTHON SOURCE LINES 237-250

.. code-block:: default


    class StyleLoss(nn.Module):

        def __init__(self, target_feature):
            super(StyleLoss, self).__init__()
            self.target = gram_matrix(target_feature).detach()

        def forward(self, input):
            G = gram_matrix(input)
            self.loss = F.mse_loss(G, self.target)
            return input









.. GENERATED FROM PYTHON SOURCE LINES 251-265

Importing the Model
-------------------

Now we need to import a pre-trained neural network. We will use a 19
layer VGG network like the one used in the paper.

PyTorch’s implementation of VGG is a module divided into two child
``Sequential`` modules: ``features`` (containing convolution and pooling layers),
and ``classifier`` (containing fully connected layers). We will use the
``features`` module because we need the output of the individual
convolution layers to measure content and style loss. Some layers have
different behavior during training than evaluation, so we must set the
network to evaluation mode using ``.eval()``.


.. GENERATED FROM PYTHON SOURCE LINES 265-270

.. code-block:: default


    cnn = models.vgg19(pretrained=True).features.to(device).eval()







.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:

    The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.

    /opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:

    Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.

    Downloading: "https://download.pytorch.org/models/vgg19-dcbb9e9d.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth

      0%|          | 0.00/548M [00:00<?, ?B/s]
      0%|          | 2.46M/548M [00:00<00:24, 23.8MB/s]
      1%|1         | 5.59M/548M [00:00<00:19, 28.4MB/s]
      2%|1         | 9.08M/548M [00:00<00:18, 31.3MB/s]
      2%|2         | 12.1M/548M [00:00<00:18, 31.0MB/s]
      3%|2         | 15.0M/548M [00:00<00:20, 26.8MB/s]
      3%|3         | 18.1M/548M [00:00<00:19, 28.4MB/s]
      4%|3         | 21.5M/548M [00:00<00:18, 30.5MB/s]
      5%|4         | 25.2M/548M [00:00<00:16, 32.5MB/s]
      5%|5         | 29.0M/548M [00:00<00:15, 34.8MB/s]
      6%|5         | 32.4M/548M [00:01<00:17, 30.5MB/s]
      6%|6         | 35.4M/548M [00:01<00:18, 29.0MB/s]
      7%|7         | 38.7M/548M [00:01<00:18, 29.3MB/s]
      8%|7         | 41.5M/548M [00:01<00:19, 27.9MB/s]
      8%|8         | 44.2M/548M [00:01<00:20, 25.8MB/s]
      9%|8         | 47.2M/548M [00:01<00:19, 26.9MB/s]
      9%|9         | 49.8M/548M [00:01<00:24, 21.4MB/s]
     10%|9         | 52.8M/548M [00:02<00:22, 23.0MB/s]
     10%|#         | 55.8M/548M [00:02<00:20, 24.8MB/s]
     11%|#         | 58.3M/548M [00:02<00:20, 25.0MB/s]
     11%|#1        | 61.0M/548M [00:02<00:19, 26.0MB/s]
     12%|#1        | 63.6M/548M [00:02<00:23, 22.1MB/s]
     12%|#2        | 68.0M/548M [00:02<00:17, 28.1MB/s]
     13%|#3        | 71.6M/548M [00:02<00:16, 30.5MB/s]
     14%|#3        | 74.7M/548M [00:02<00:16, 29.4MB/s]
     14%|#4        | 78.2M/548M [00:02<00:15, 30.9MB/s]
     15%|#4        | 81.3M/548M [00:03<00:16, 29.8MB/s]
     15%|#5        | 84.9M/548M [00:03<00:15, 32.1MB/s]
     16%|#6        | 89.5M/548M [00:03<00:13, 36.5MB/s]
     17%|#7        | 93.5M/548M [00:03<00:12, 37.6MB/s]
     18%|#7        | 97.1M/548M [00:03<00:14, 32.9MB/s]
     18%|#8        | 100M/548M [00:03<00:15, 30.4MB/s] 
     19%|#8        | 103M/548M [00:03<00:18, 25.1MB/s]
     19%|#9        | 106M/548M [00:03<00:18, 25.7MB/s]
     20%|#9        | 109M/548M [00:04<00:18, 25.4MB/s]
     20%|##        | 112M/548M [00:04<00:16, 27.1MB/s]
     21%|##        | 115M/548M [00:04<00:16, 27.3MB/s]
     21%|##1       | 117M/548M [00:04<00:18, 24.6MB/s]
     22%|##1       | 120M/548M [00:04<00:17, 25.7MB/s]
     23%|##2       | 124M/548M [00:04<00:14, 29.8MB/s]
     23%|##3       | 127M/548M [00:04<00:14, 31.3MB/s]
     24%|##3       | 130M/548M [00:04<00:14, 30.3MB/s]
     24%|##4       | 134M/548M [00:04<00:13, 32.3MB/s]
     25%|##5       | 137M/548M [00:05<00:13, 32.4MB/s]
     26%|##5       | 140M/548M [00:05<00:13, 32.2MB/s]
     26%|##6       | 143M/548M [00:05<00:13, 31.4MB/s]
     27%|##6       | 146M/548M [00:05<00:13, 30.6MB/s]
     27%|##7       | 149M/548M [00:05<00:14, 29.9MB/s]
     28%|##7       | 152M/548M [00:05<00:14, 28.1MB/s]
     28%|##8       | 155M/548M [00:05<00:15, 26.8MB/s]
     29%|##8       | 158M/548M [00:05<00:15, 26.4MB/s]
     29%|##9       | 161M/548M [00:05<00:14, 27.8MB/s]
     30%|###       | 165M/548M [00:05<00:12, 32.0MB/s]
     31%|###       | 168M/548M [00:06<00:13, 29.8MB/s]
     31%|###1      | 171M/548M [00:06<00:14, 26.9MB/s]
     32%|###1      | 174M/548M [00:06<00:13, 28.8MB/s]
     32%|###2      | 177M/548M [00:06<00:12, 30.3MB/s]
     33%|###2      | 180M/548M [00:06<00:14, 26.3MB/s]
     33%|###3      | 183M/548M [00:06<00:14, 25.8MB/s]
     34%|###3      | 185M/548M [00:06<00:16, 22.7MB/s]
     34%|###4      | 188M/548M [00:06<00:15, 24.6MB/s]
     35%|###4      | 191M/548M [00:07<00:14, 25.8MB/s]
     35%|###5      | 195M/548M [00:07<00:13, 27.9MB/s]
     36%|###6      | 198M/548M [00:07<00:12, 28.7MB/s]
     37%|###6      | 201M/548M [00:07<00:12, 29.4MB/s]
     37%|###7      | 204M/548M [00:07<00:11, 31.9MB/s]
     38%|###7      | 208M/548M [00:07<00:10, 34.0MB/s]
     39%|###8      | 211M/548M [00:07<00:10, 33.9MB/s]
     39%|###9      | 215M/548M [00:07<00:10, 32.4MB/s]
     40%|###9      | 218M/548M [00:07<00:11, 30.9MB/s]
     40%|####      | 221M/548M [00:08<00:12, 28.0MB/s]
     41%|####      | 224M/548M [00:08<00:11, 30.2MB/s]
     42%|####1     | 228M/548M [00:08<00:10, 32.5MB/s]
     42%|####2     | 232M/548M [00:08<00:09, 36.6MB/s]
     43%|####3     | 236M/548M [00:08<00:09, 34.4MB/s]
     44%|####3     | 239M/548M [00:08<00:09, 34.8MB/s]
     44%|####4     | 243M/548M [00:08<00:09, 32.2MB/s]
     45%|####4     | 246M/548M [00:08<00:09, 33.1MB/s]
     46%|####5     | 250M/548M [00:08<00:08, 36.0MB/s]
     46%|####6     | 254M/548M [00:09<00:09, 34.1MB/s]
     47%|####7     | 258M/548M [00:09<00:08, 36.8MB/s]
     48%|####7     | 263M/548M [00:09<00:07, 40.6MB/s]
     49%|####8     | 267M/548M [00:09<00:07, 38.6MB/s]
     49%|####9     | 271M/548M [00:09<00:09, 30.2MB/s]
     50%|####9     | 274M/548M [00:09<00:09, 30.5MB/s]
     51%|#####     | 277M/548M [00:09<00:11, 24.0MB/s]
     51%|#####     | 280M/548M [00:10<00:11, 24.0MB/s]
     52%|#####1    | 283M/548M [00:10<00:10, 26.9MB/s]
     53%|#####2    | 288M/548M [00:10<00:08, 34.0MB/s]
     53%|#####3    | 292M/548M [00:10<00:07, 34.7MB/s]
     54%|#####3    | 295M/548M [00:10<00:08, 29.8MB/s]
     55%|#####4    | 299M/548M [00:10<00:08, 31.9MB/s]
     55%|#####5    | 302M/548M [00:10<00:08, 30.8MB/s]
     56%|#####5    | 305M/548M [00:10<00:08, 31.0MB/s]
     56%|#####6    | 309M/548M [00:10<00:08, 30.0MB/s]
     57%|#####6    | 311M/548M [00:11<00:10, 23.8MB/s]
     57%|#####7    | 314M/548M [00:11<00:09, 25.2MB/s]
     58%|#####7    | 317M/548M [00:11<00:09, 26.7MB/s]
     59%|#####8    | 321M/548M [00:11<00:08, 28.9MB/s]
     59%|#####9    | 324M/548M [00:11<00:08, 27.0MB/s]
     60%|#####9    | 326M/548M [00:11<00:09, 24.6MB/s]
     60%|######    | 330M/548M [00:11<00:08, 26.8MB/s]
     61%|######    | 332M/548M [00:11<00:08, 27.4MB/s]
     61%|######1   | 336M/548M [00:12<00:07, 29.2MB/s]
     62%|######1   | 339M/548M [00:12<00:07, 29.6MB/s]
     62%|######2   | 342M/548M [00:12<00:07, 29.7MB/s]
     63%|######3   | 345M/548M [00:12<00:06, 32.3MB/s]
     64%|######3   | 348M/548M [00:12<00:07, 27.7MB/s]
     64%|######4   | 352M/548M [00:12<00:06, 31.2MB/s]
     65%|######4   | 355M/548M [00:12<00:06, 29.3MB/s]
     65%|######5   | 359M/548M [00:12<00:06, 30.4MB/s]
     66%|######6   | 363M/548M [00:12<00:05, 34.4MB/s]
     67%|######6   | 367M/548M [00:13<00:05, 35.1MB/s]
     68%|######7   | 371M/548M [00:13<00:04, 37.5MB/s]
     68%|######8   | 374M/548M [00:13<00:06, 28.4MB/s]
     69%|######9   | 379M/548M [00:13<00:05, 31.9MB/s]
     70%|######9   | 382M/548M [00:13<00:05, 30.2MB/s]
     70%|#######   | 385M/548M [00:13<00:05, 29.8MB/s]
     71%|#######   | 388M/548M [00:13<00:06, 26.0MB/s]
     71%|#######1  | 391M/548M [00:13<00:06, 27.0MB/s]
     72%|#######2  | 395M/548M [00:14<00:05, 28.8MB/s]
     73%|#######2  | 398M/548M [00:14<00:05, 29.3MB/s]
     73%|#######3  | 402M/548M [00:14<00:04, 33.1MB/s]
     74%|#######3  | 405M/548M [00:14<00:04, 32.0MB/s]
     74%|#######4  | 408M/548M [00:14<00:04, 31.5MB/s]
     75%|#######5  | 412M/548M [00:14<00:04, 34.1MB/s]
     76%|#######5  | 416M/548M [00:14<00:03, 35.9MB/s]
     77%|#######6  | 419M/548M [00:14<00:04, 31.2MB/s]
     77%|#######7  | 422M/548M [00:14<00:04, 30.3MB/s]
     78%|#######7  | 426M/548M [00:15<00:03, 32.3MB/s]
     78%|#######8  | 429M/548M [00:15<00:03, 31.6MB/s]
     79%|#######8  | 432M/548M [00:15<00:04, 27.1MB/s]
     79%|#######9  | 435M/548M [00:15<00:04, 27.5MB/s]
     80%|#######9  | 438M/548M [00:15<00:04, 27.7MB/s]
     80%|########  | 441M/548M [00:15<00:04, 26.6MB/s]
     81%|########  | 443M/548M [00:15<00:04, 26.8MB/s]
     81%|########1 | 446M/548M [00:15<00:04, 26.7MB/s]
     82%|########1 | 449M/548M [00:15<00:03, 27.1MB/s]
     82%|########2 | 451M/548M [00:16<00:04, 25.3MB/s]
     83%|########2 | 455M/548M [00:16<00:03, 27.7MB/s]
     83%|########3 | 457M/548M [00:16<00:04, 22.5MB/s]
     84%|########3 | 460M/548M [00:16<00:03, 23.3MB/s]
     84%|########4 | 462M/548M [00:16<00:04, 21.8MB/s]
     85%|########4 | 464M/548M [00:16<00:04, 21.3MB/s]
     85%|########5 | 467M/548M [00:16<00:03, 22.4MB/s]
     86%|########5 | 469M/548M [00:16<00:03, 22.6MB/s]
     86%|########6 | 472M/548M [00:17<00:03, 23.6MB/s]
     87%|########6 | 474M/548M [00:17<00:03, 23.9MB/s]
     87%|########6 | 477M/548M [00:17<00:03, 24.4MB/s]
     87%|########7 | 479M/548M [00:17<00:03, 23.1MB/s]
     88%|########7 | 482M/548M [00:17<00:02, 24.4MB/s]
     88%|########8 | 484M/548M [00:17<00:02, 24.1MB/s]
     89%|########8 | 486M/548M [00:17<00:02, 23.3MB/s]
     89%|########9 | 489M/548M [00:17<00:02, 23.4MB/s]
     90%|########9 | 491M/548M [00:17<00:02, 25.3MB/s]
     90%|######### | 495M/548M [00:17<00:02, 27.2MB/s]
     91%|######### | 497M/548M [00:18<00:01, 27.1MB/s]
     91%|#########1| 500M/548M [00:18<00:02, 23.5MB/s]
     92%|#########1| 502M/548M [00:18<00:04, 12.0MB/s]
     92%|#########1| 504M/548M [00:18<00:03, 12.7MB/s]
     92%|#########2| 506M/548M [00:18<00:03, 13.9MB/s]
     93%|#########2| 508M/548M [00:19<00:02, 16.2MB/s]
     93%|#########3| 512M/548M [00:19<00:01, 21.0MB/s]
     94%|#########3| 514M/548M [00:19<00:01, 22.2MB/s]
     94%|#########4| 517M/548M [00:19<00:01, 23.5MB/s]
     95%|#########5| 521M/548M [00:19<00:01, 27.8MB/s]
     96%|#########5| 524M/548M [00:19<00:00, 28.2MB/s]
     96%|#########6| 527M/548M [00:19<00:00, 25.7MB/s]
     97%|#########6| 531M/548M [00:19<00:00, 31.5MB/s]
     98%|#########7| 535M/548M [00:19<00:00, 33.1MB/s]
     98%|#########8| 538M/548M [00:20<00:00, 31.1MB/s]
     99%|#########8| 541M/548M [00:20<00:00, 30.0MB/s]
     99%|#########9| 545M/548M [00:20<00:00, 32.8MB/s]
    100%|##########| 548M/548M [00:20<00:00, 28.2MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 271-275

Additionally, VGG networks are trained on images with each channel
normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].
We will use them to normalize the image before sending it into the network.


.. GENERATED FROM PYTHON SOURCE LINES 275-295

.. code-block:: default


    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

    # create a module to normalize input image so we can easily put it in a
    # nn.Sequential
    class Normalization(nn.Module):
        def __init__(self, mean, std):
            super(Normalization, self).__init__()
            # .view the mean and std to make them [C x 1 x 1] so that they can
            # directly work with image Tensor of shape [B x C x H x W].
            # B is batch size. C is number of channels. H is height and W is width.
            self.mean = torch.tensor(mean).view(-1, 1, 1)
            self.std = torch.tensor(std).view(-1, 1, 1)

        def forward(self, img):
            # normalize img
            return (img - self.mean) / self.std









.. GENERATED FROM PYTHON SOURCE LINES 296-303

A ``Sequential`` module contains an ordered list of child modules. For
instance, ``vgg19.features`` contains a sequence (Conv2d, ReLU, MaxPool2d,
Conv2d, ReLU…) aligned in the right order of depth. We need to add our
content loss and style loss layers immediately after the convolution
layer they are detecting. To do this we must create a new ``Sequential``
module that has content loss and style loss modules correctly inserted.


.. GENERATED FROM PYTHON SOURCE LINES 303-368

.. code-block:: default


    # desired depth layers to compute style/content losses :
    content_layers_default = ['conv_4']
    style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

    def get_style_model_and_losses(cnn, normalization_mean, normalization_std,
                                   style_img, content_img,
                                   content_layers=content_layers_default,
                                   style_layers=style_layers_default):
        # normalization module
        normalization = Normalization(normalization_mean, normalization_std).to(device)

        # just in order to have an iterable access to or list of content/syle
        # losses
        content_losses = []
        style_losses = []

        # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential
        # to put in modules that are supposed to be activated sequentially
        model = nn.Sequential(normalization)

        i = 0  # increment every time we see a conv
        for layer in cnn.children():
            if isinstance(layer, nn.Conv2d):
                i += 1
                name = 'conv_{}'.format(i)
            elif isinstance(layer, nn.ReLU):
                name = 'relu_{}'.format(i)
                # The in-place version doesn't play very nicely with the ContentLoss
                # and StyleLoss we insert below. So we replace with out-of-place
                # ones here.
                layer = nn.ReLU(inplace=False)
            elif isinstance(layer, nn.MaxPool2d):
                name = 'pool_{}'.format(i)
            elif isinstance(layer, nn.BatchNorm2d):
                name = 'bn_{}'.format(i)
            else:
                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

            model.add_module(name, layer)

            if name in content_layers:
                # add content loss:
                target = model(content_img).detach()
                content_loss = ContentLoss(target)
                model.add_module("content_loss_{}".format(i), content_loss)
                content_losses.append(content_loss)

            if name in style_layers:
                # add style loss:
                target_feature = model(style_img).detach()
                style_loss = StyleLoss(target_feature)
                model.add_module("style_loss_{}".format(i), style_loss)
                style_losses.append(style_loss)

        # now we trim off the layers after the last content and style losses
        for i in range(len(model) - 1, -1, -1):
            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
                break

        model = model[:(i + 1)]

        return model, style_losses, content_losses









.. GENERATED FROM PYTHON SOURCE LINES 369-372

Next, we select the input image. You can use a copy of the content image
or white noise.


.. GENERATED FROM PYTHON SOURCE LINES 372-382

.. code-block:: default


    input_img = content_img.clone()
    # if you want to use white noise instead uncomment the below line:
    # input_img = torch.randn(content_img.data.size(), device=device)

    # add the original input image to the figure:
    plt.figure()
    imshow(input_img, title='Input Image')





.. image-sg:: /advanced/images/sphx_glr_neural_style_tutorial_003.png
   :alt: Input Image
   :srcset: /advanced/images/sphx_glr_neural_style_tutorial_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 383-392

Gradient Descent
----------------

As Leon Gatys, the author of the algorithm, suggested `here <https://discuss.pytorch.org/t/pytorch-tutorial-for-neural-transfert-of-artistic-style/336/20?u=alexis-jacq>`__, we will use
L-BFGS algorithm to run our gradient descent. Unlike training a network,
we want to train the input image in order to minimise the content/style
losses. We will create a PyTorch L-BFGS optimizer ``optim.LBFGS`` and pass
our image to it as the tensor to optimize.


.. GENERATED FROM PYTHON SOURCE LINES 392-399

.. code-block:: default


    def get_input_optimizer(input_img):
        # this line to show that input is a parameter that requires a gradient
        optimizer = optim.LBFGS([input_img])
        return optimizer









.. GENERATED FROM PYTHON SOURCE LINES 400-411

Finally, we must define a function that performs the neural transfer. For
each iteration of the networks, it is fed an updated input and computes
new losses. We will run the ``backward`` methods of each loss module to
dynamicaly compute their gradients. The optimizer requires a “closure”
function, which reevaluates the module and returns the loss.

We still have one final constraint to address. The network may try to
optimize the input with values that exceed the 0 to 1 tensor range for
the image. We can address this by correcting the input values to be
between 0 to 1 each time the network is run.


.. GENERATED FROM PYTHON SOURCE LINES 411-470

.. code-block:: default


    def run_style_transfer(cnn, normalization_mean, normalization_std,
                           content_img, style_img, input_img, num_steps=300,
                           style_weight=1000000, content_weight=1):
        """Run the style transfer."""
        print('Building the style transfer model..')
        model, style_losses, content_losses = get_style_model_and_losses(cnn,
            normalization_mean, normalization_std, style_img, content_img)

        # We want to optimize the input and not the model parameters so we
        # update all the requires_grad fields accordingly
        input_img.requires_grad_(True)
        model.requires_grad_(False)

        optimizer = get_input_optimizer(input_img)

        print('Optimizing..')
        run = [0]
        while run[0] <= num_steps:

            def closure():
                # correct the values of updated input image
                with torch.no_grad():
                    input_img.clamp_(0, 1)

                optimizer.zero_grad()
                model(input_img)
                style_score = 0
                content_score = 0

                for sl in style_losses:
                    style_score += sl.loss
                for cl in content_losses:
                    content_score += cl.loss

                style_score *= style_weight
                content_score *= content_weight

                loss = style_score + content_score
                loss.backward()

                run[0] += 1
                if run[0] % 50 == 0:
                    print("run {}:".format(run))
                    print('Style Loss : {:4f} Content Loss: {:4f}'.format(
                        style_score.item(), content_score.item()))
                    print()

                return style_score + content_score

            optimizer.step(closure)

        # a last correction...
        with torch.no_grad():
            input_img.clamp_(0, 1)

        return input_img









.. GENERATED FROM PYTHON SOURCE LINES 471-473

Finally, we can run the algorithm.


.. GENERATED FROM PYTHON SOURCE LINES 473-484

.. code-block:: default


    output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,
                                content_img, style_img, input_img)

    plt.figure()
    imshow(output, title='Output Image')

    # sphinx_gallery_thumbnail_number = 4
    plt.ioff()
    plt.show()




.. image-sg:: /advanced/images/sphx_glr_neural_style_tutorial_004.png
   :alt: Output Image
   :srcset: /advanced/images/sphx_glr_neural_style_tutorial_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Building the style transfer model..
    /var/lib/jenkins/workspace/advanced_source/neural_style_tutorial.py:287: UserWarning:

    To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

    /var/lib/jenkins/workspace/advanced_source/neural_style_tutorial.py:288: UserWarning:

    To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

    Optimizing..
    run [50]:
    Style Loss : 4.188396 Content Loss: 4.153408

    run [100]:
    Style Loss : 1.159103 Content Loss: 3.051302

    run [150]:
    Style Loss : 0.716885 Content Loss: 2.653880

    run [200]:
    Style Loss : 0.479834 Content Loss: 2.493725

    run [250]:
    Style Loss : 0.345357 Content Loss: 2.402390

    run [300]:
    Style Loss : 0.262762 Content Loss: 2.349280






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  59.312 seconds)


.. _sphx_glr_download_advanced_neural_style_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_style_tutorial.py <neural_style_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_style_tutorial.ipynb <neural_style_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
