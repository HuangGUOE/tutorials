
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Text-to-speech with torchaudio — PyTorch Tutorials 1.12.1+cu102 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css" rel="stylesheet" type="text/css"/>
<link href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="forced_alignment_with_torchaudio_tutorial.html" rel="next" title="Forced Alignment with Wav2Vec2"/>
<link href="speech_command_classification_with_torchaudio_tutorial.html" rel="prev" title="Speech Command Classification with torchaudio"/>
<!-- Google Analytics -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
<script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
<!-- End Google Analytics -->
<script src="../_static/js/modernizr.min.js"></script>
<!-- Preload the theme fonts -->
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
<!-- Preload the katex fonts -->
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
</head>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="container">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/ecosystem">Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/mobile">Mobile</a>
</li>
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li class="active">
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="resource-option with-down-orange-arrow">
                Docs
              </a>
<div class="resources-dropdown-menu">
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
<span class="dropdown-title">torchaudio</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
<span class="dropdown-title">torchtext</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
<span class="dropdown-title">torchvision</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
<span class="dropdown-title">TorchRec</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
<span class="dropdown-title">TorchData</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
<span class="dropdown-title">TorchServe</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
<span class="dropdown-title">TorchX</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
<span class="dropdown-title">PyTorch on XLA Devices</span>
<p></p>
</a>
</div>
</div></li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="resource-option with-down-arrow">
                Resources
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/features">
<span class="dropdown-title">About</span>
<p>Learn about PyTorch’s features and capabilities</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
<span class="dropdown-title">Community</span>
<p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
<p>Find resources and get questions answered</p>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
<span class="dropdown-title">Forums</span>
<p>A place to discuss PyTorch code, issues, install, research</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/hub">
<span class="dropdown-title">Models (Beta)</span>
<p>Discover, publish, and reuse pre-trained models</p>
</a>
</div>
</div>
</li>
<li>
<a href="https://github.com/pytorch/pytorch">GitHub</a>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#"></a>
</div>
</div>
</div>
<body class="pytorch-body">
<div class="table-of-contents-link-wrapper">
<span>Table of Contents</span>
<a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#"></a>
</div>
<nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
<div class="pytorch-side-scroll">
<div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
<div class="pytorch-left-menu-search">
<div class="version">
                  1.12.1+cu102
                </div>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search Tutorials" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<p class="caption" role="heading"><span class="caption-text">PyTorch Recipes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes/recipes_index.html">See All Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prototype/prototype_index.html">See All Prototype Recipes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/quickstart_tutorial.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/tensorqs_tutorial.html">Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/transforms_tutorial.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/buildmodel_tutorial.html">Build the Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/autogradqs_tutorial.html">Automatic Differentiation with <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/optimization_tutorial.html">Optimizing Model Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/saveloadrun_tutorial.html">Save and Load the Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch on YouTube</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt.html">Introduction to PyTorch - YouTube Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/introyt1_tutorial.html">Introduction to PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/tensors_deeper_tutorial.html">Introduction to PyTorch Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learning PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/nn_tutorial.html">What is <cite>torch.nn</cite> <em>really</em>?</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_tutorial.html">Visualizing Models, Data, and Training with TensorBoard</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Image and Video</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">Adversarial Example Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/vt_tutorial.html">Optimizing Vision Transformer Model for Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Audio</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_io_tutorial.html">Audio I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_resampling_tutorial.html">Audio Resampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_data_augmentation_tutorial.html">Audio Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_feature_extractions_tutorial.html">Audio Feature Extractions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_feature_augmentation_tutorial.html">Audio Feature Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_datasets_tutorial.html">Audio Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech_recognition_pipeline_tutorial.html">Speech Recognition with Wav2Vec2</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech_command_classification_with_torchaudio_tutorial.html">Speech Command Classification with torchaudio</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Text-to-speech with torchaudio</a></li>
<li class="toctree-l1"><a class="reference internal" href="forced_alignment_with_torchaudio_tutorial.html">Forced Alignment with Wav2Vec2</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Text</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transformer_tutorial.html">Language Modeling with nn.Transformer and TorchText</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/bettertransformer_tutorial.html">Fast Transformer Inference with Better Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_classification_tutorial.html">NLP From Scratch: Classifying Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_generation_tutorial.html">NLP From Scratch: Generating Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq_translation_tutorial.html">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/text_sentiment_ngrams_tutorial.html">Text classification with the torchtext library</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mario_rl_tutorial.html">Train a Mario-playing RL Agent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deploying PyTorch Models in Production</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/Intro_to_TorchScript_tutorial.html">Introduction to TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_export.html">Loading a TorchScript Model in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="realtime_rpi.html">Real Time Inference on Raspberry Pi 4 (30 fps!)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_autograd.html">Autograd in C++ Frontend</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/vt_tutorial.html">Optimizing Vision Transformer Model for Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="pruning_tutorial.html">Pruning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_quantization_bert_tutorial.html">(beta) Dynamic Quantization on BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized_transfer_learning_tutorial.html">(beta) Quantized Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchserve_with_ipex.html">Grokking PyTorch Intel CPU performance from first principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="nvfuser_intro_tutorial.html">Getting Started - Accelerate Your Scripts with nvFuser</a></li>
<li class="toctree-l1"><a class="reference internal" href="ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallel and Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/ddp_series_intro.html">Distributed Data Parallel in PyTorch - Video Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_parallel_tutorial.html">Single-Machine Model Parallel Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel(FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_adavnced_tutorial.html">Advanced Model Training with Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="process_group_cpp_extension_tutorial.html">Customize Process Group Backends Using Cpp Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_tutorial.html">Training Transformer models using Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mobile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deeplabv3_on_ios.html">Image Segmentation DeepLabV3 on iOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deeplabv3_on_android.html">Image Segmentation DeepLabV3 on Android</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Recommendation Systems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchrec_tutorial.html">Introduction to TorchRec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
</div>
</div>
</nav>
<div class="pytorch-container">
<div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
<div class="pytorch-breadcrumbs-wrapper">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="pytorch-breadcrumbs">
<li>
<a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>
<li>Text-to-speech with torchaudio</li>
<li class="pytorch-breadcrumbs-aside">
<a href="../_sources/intermediate/text_to_speech_with_torchaudio.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"/></a>
</li>
</ul>
</div>
</div>
<div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
</div>
<section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
<div class="pytorch-content-left">
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/text_to_speech_with_torchaudio</div>
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</div>
<div class="rst-content">
<div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-intermediate-text-to-speech-with-torchaudio-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="text-to-speech-with-torchaudio">
<span id="sphx-glr-intermediate-text-to-speech-with-torchaudio-py"></span><h1>Text-to-speech with torchaudio<a class="headerlink" href="#text-to-speech-with-torchaudio" title="Permalink to this heading">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/yangarbiter">Yao-Yuan Yang</a>, <a class="reference external" href="mailto:moto%40fb.com">Moto
Hira</a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>This tutorial shows how to build text-to-speech pipeline, using the
pretrained Tacotron2 in torchaudio.</p>
<p>The text-to-speech pipeline goes as follows: 1. Text preprocessing</p>
<p>First, the input text is encoded into a list of symbols. In this
tutorial, we will use English characters and phonemes as the symbols.</p>
<ol class="arabic simple" start="2">
<li><p>Spectrogram generation</p></li>
</ol>
<p>From the encoded text, a spectrogram is generated. We use <code class="docutils literal notranslate"><span class="pre">Tacotron2</span></code>
model for this.</p>
<ol class="arabic simple" start="3">
<li><p>Time-domain conversion</p></li>
</ol>
<p>The last step is converting the spectrogram into the waveform. The
process to generate speech from spectrogram is also called Vocoder. In
this tutorial, three different vocoders are used,
<code class="docutils literal notranslate"><span class="pre">`WaveRNN</span></code> &lt;<a class="reference external" href="https://pytorch.org/audio/stable/models/wavernn.html">https://pytorch.org/audio/stable/models/wavernn.html</a>&gt;`__,
<code class="docutils literal notranslate"><span class="pre">`Griffin-Lim</span></code> &lt;<a class="reference external" href="https://pytorch.org/audio/stable/transforms.html#griffinlim">https://pytorch.org/audio/stable/transforms.html#griffinlim</a>&gt;`__,
and
<code class="docutils literal notranslate"><span class="pre">`Nvidia's</span> <span class="pre">WaveGlow</span></code> &lt;<a class="reference external" href="https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/">https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/</a>&gt;`__.</p>
<p>The following figure illustrates the whole process.</p>
<img alt="https://download.pytorch.org/torchaudio/tutorial-assets/tacotron2_tts_pipeline.png" src="https://download.pytorch.org/torchaudio/tutorial-assets/tacotron2_tts_pipeline.png"/>
</div>
<div class="section" id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this heading">¶</a></h2>
<p>First, we install the necessary dependencies. In addition to
<code class="docutils literal notranslate"><span class="pre">torchaudio</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepPhonemizer</span></code> is required to perform phoneme-based
encoding.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%%bash
pip3 install deep_phonemizer
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchaudio</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">IPython</span>

<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torchaudio</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>1.12.1+cu102
0.12.1+cu102
</pre></div>
</div>
</div>
<div class="section" id="text-processing">
<h2>Text Processing<a class="headerlink" href="#text-processing" title="Permalink to this heading">¶</a></h2>
<div class="section" id="character-based-encoding">
<h3>Character-based encoding<a class="headerlink" href="#character-based-encoding" title="Permalink to this heading">¶</a></h3>
<p>In this section, we will go through how the character-based encoding
works.</p>
<p>Since the pre-trained Tacotron2 model expects specific set of symbol
tables, the same functionalities available in <code class="docutils literal notranslate"><span class="pre">torchaudio</span></code>. This
section is more for the explanation of the basis of encoding.</p>
<p>Firstly, we define the set of symbols. For example, we can use
<code class="docutils literal notranslate"><span class="pre">'_-!\'(),.:;?</span> <span class="pre">abcdefghijklmnopqrstuvwxyz'</span></code>. Then, we will map the
each character of the input text into the index of the corresponding
symbol in the table.</p>
<p>The following is an example of such processing. In the example, symbols
that are not in the table are ignored.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">symbols</span> <span class="o">=</span> <span class="s1">'_-!</span><span class="se">\'</span><span class="s1">(),.:;? abcdefghijklmnopqrstuvwxyz'</span>
<span class="n">look_up</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">symbols</span><span class="p">)}</span>
<span class="n">symbols</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">text_to_sequence</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">look_up</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">text</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">]</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello world! Text to speech!"</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text_to_sequence</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[19, 16, 23, 23, 26, 11, 34, 26, 29, 23, 15, 2, 11, 31, 16, 35, 31, 11, 31, 26, 11, 30, 27, 16, 16, 14, 19, 2]
</pre></div>
</div>
<p>As mentioned in the above, the symbol table and indices must match
what the pretrained Tacotron2 model expects. <code class="docutils literal notranslate"><span class="pre">torchaudio</span></code> provides the
transform along with the pretrained model. For example, you can
instantiate and use such transform as follow.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">processor</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor" title="torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor"><span class="n">torchaudio</span><span class="o">.</span><span class="n">pipelines</span><span class="o">.</span><span class="n">TACOTRON2_WAVERNN_CHAR_LJSPEECH</span><span class="o">.</span><span class="n">get_text_processor</span></a><span class="p">()</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello world! Text to speech!"</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[19, 16, 23, 23, 26, 11, 34, 26, 29, 23, 15,  2, 11, 31, 16, 35, 31, 11,
         31, 26, 11, 30, 27, 16, 16, 14, 19,  2]])
tensor([28], dtype=torch.int32)
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">processor</span></code> object takes either a text or list of texts as inputs.
When a list of texts are provided, the returned <code class="docutils literal notranslate"><span class="pre">lengths</span></code> variable
represents the valid length of each processed tokens in the output
batch.</p>
<p>The intermediate representation can be retrieved as follow.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">([</span><a class="sphx-glr-backref-module-torchaudio-pipelines-Tacotron2TTSBundle sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.TextProcessor.tokens" title="torchaudio.pipelines.Tacotron2TTSBundle.TextProcessor.tokens"><span class="n">processor</span><span class="o">.</span><span class="n">tokens</span></a><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]]])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 't', 'e', 'x', 't', ' ', 't', 'o', ' ', 's', 'p', 'e', 'e', 'c', 'h', '!']
</pre></div>
</div>
</div>
<div class="section" id="phoneme-based-encoding">
<h3>Phoneme-based encoding<a class="headerlink" href="#phoneme-based-encoding" title="Permalink to this heading">¶</a></h3>
<p>Phoneme-based encoding is similar to character-based encoding, but it
uses a symbol table based on phonemes and a G2P (Grapheme-to-Phoneme)
model.</p>
<p>The detail of the G2P model is out of scope of this tutorial, we will
just look at what the conversion looks like.</p>
<p>Similar to the case of character-based encoding, the encoding process is
expected to match what a pretrained Tacotron2 model is trained on.
<code class="docutils literal notranslate"><span class="pre">torchaudio</span></code> has an interface to create the process.</p>
<p>The following code illustrates how to make and use the process. Behind
the scene, a G2P model is created using <code class="docutils literal notranslate"><span class="pre">DeepPhonemizer</span></code> package, and
the pretrained weights published by the author of <code class="docutils literal notranslate"><span class="pre">DeepPhonemizer</span></code> is
fetched.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bundle</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-data" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH" title="torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH"><span class="n">torchaudio</span><span class="o">.</span><span class="n">pipelines</span><span class="o">.</span><span class="n">TACOTRON2_WAVERNN_PHONE_LJSPEECH</span></a>

<span class="n">processor</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor" title="torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor"><span class="n">bundle</span><span class="o">.</span><span class="n">get_text_processor</span></a><span class="p">()</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello world! Text to speech!"</span>
<span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode" title="torch.inference_mode"><span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span></a><span class="p">():</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>  0%|          | 0.00/63.6M [00:00&lt;?, ?B/s]
  0%|          | 8.00k/63.6M [00:00&lt;14:45, 75.4kB/s]
  0%|          | 48.0k/63.6M [00:00&lt;04:24, 252kB/s]
  0%|          | 112k/63.6M [00:00&lt;02:42, 410kB/s]
  0%|          | 248k/63.6M [00:00&lt;01:28, 751kB/s]
  1%|          | 352k/63.6M [00:00&lt;01:19, 830kB/s]
  1%|          | 472k/63.6M [00:00&lt;01:11, 930kB/s]
  1%|          | 592k/63.6M [00:00&lt;01:06, 993kB/s]
  1%|1         | 712k/63.6M [00:00&lt;01:03, 1.03MB/s]
  1%|1         | 848k/63.6M [00:00&lt;00:59, 1.11MB/s]
  1%|1         | 976k/63.6M [00:01&lt;00:57, 1.14MB/s]
  2%|1         | 1.07M/63.6M [00:01&lt;00:57, 1.13MB/s]
  2%|1         | 1.20M/63.6M [00:01&lt;00:55, 1.17MB/s]
  2%|2         | 1.36M/63.6M [00:01&lt;00:51, 1.27MB/s]
  2%|2         | 1.49M/63.6M [00:01&lt;00:51, 1.27MB/s]
  3%|2         | 1.62M/63.6M [00:01&lt;00:51, 1.27MB/s]
  3%|2         | 1.77M/63.6M [00:01&lt;00:49, 1.32MB/s]
  3%|2         | 1.91M/63.6M [00:01&lt;00:49, 1.30MB/s]
  3%|3         | 2.05M/63.6M [00:01&lt;00:48, 1.34MB/s]
  3%|3         | 2.20M/63.6M [00:02&lt;00:47, 1.36MB/s]
  4%|3         | 2.35M/63.6M [00:02&lt;00:46, 1.38MB/s]
  4%|3         | 2.50M/63.6M [00:02&lt;00:45, 1.39MB/s]
  4%|4         | 2.65M/63.6M [00:02&lt;00:45, 1.40MB/s]
  4%|4         | 2.80M/63.6M [00:02&lt;00:45, 1.41MB/s]
  5%|4         | 2.97M/63.6M [00:02&lt;00:43, 1.48MB/s]
  5%|4         | 3.12M/63.6M [00:02&lt;00:43, 1.46MB/s]
  5%|5         | 3.27M/63.6M [00:02&lt;00:43, 1.45MB/s]
  5%|5         | 3.43M/63.6M [00:02&lt;00:42, 1.48MB/s]
  6%|5         | 3.58M/63.6M [00:03&lt;00:42, 1.47MB/s]
  6%|5         | 3.75M/63.6M [00:03&lt;00:41, 1.52MB/s]
  6%|6         | 3.90M/63.6M [00:03&lt;00:41, 1.49MB/s]
  6%|6         | 4.06M/63.6M [00:03&lt;00:41, 1.52MB/s]
  7%|6         | 4.23M/63.6M [00:03&lt;00:40, 1.53MB/s]
  7%|6         | 4.38M/63.6M [00:03&lt;00:41, 1.50MB/s]
  7%|7         | 4.55M/63.6M [00:03&lt;00:40, 1.54MB/s]
  7%|7         | 4.71M/63.6M [00:03&lt;00:39, 1.55MB/s]
  8%|7         | 4.86M/63.6M [00:03&lt;00:40, 1.52MB/s]
  8%|7         | 5.02M/63.6M [00:04&lt;00:40, 1.53MB/s]
  8%|8         | 5.19M/63.6M [00:04&lt;00:39, 1.54MB/s]
  8%|8         | 5.36M/63.6M [00:04&lt;00:38, 1.57MB/s]
  9%|8         | 5.52M/63.6M [00:04&lt;00:39, 1.55MB/s]
  9%|8         | 5.67M/63.6M [00:04&lt;00:39, 1.54MB/s]
  9%|9         | 5.84M/63.6M [00:04&lt;00:39, 1.55MB/s]
  9%|9         | 6.01M/63.6M [00:04&lt;00:38, 1.58MB/s]
 10%|9         | 6.16M/63.6M [00:04&lt;00:38, 1.55MB/s]
 10%|9         | 6.32M/63.6M [00:04&lt;00:39, 1.54MB/s]
 10%|#         | 6.48M/63.6M [00:05&lt;00:38, 1.55MB/s]
 10%|#         | 6.65M/63.6M [00:05&lt;00:38, 1.55MB/s]
 11%|#         | 6.80M/63.6M [00:05&lt;00:38, 1.54MB/s]
 11%|#         | 6.97M/63.6M [00:05&lt;00:38, 1.55MB/s]
 11%|#1        | 7.13M/63.6M [00:05&lt;00:38, 1.55MB/s]
 11%|#1        | 7.30M/63.6M [00:05&lt;00:37, 1.56MB/s]
 12%|#1        | 7.46M/63.6M [00:05&lt;00:37, 1.57MB/s]
 12%|#1        | 7.62M/63.6M [00:05&lt;00:38, 1.55MB/s]
 12%|#2        | 7.78M/63.6M [00:05&lt;00:37, 1.55MB/s]
 12%|#2        | 7.95M/63.6M [00:06&lt;00:37, 1.56MB/s]
 13%|#2        | 8.11M/63.6M [00:06&lt;00:37, 1.56MB/s]
 13%|#2        | 8.27M/63.6M [00:06&lt;00:37, 1.55MB/s]
 13%|#3        | 8.43M/63.6M [00:06&lt;00:37, 1.55MB/s]
 14%|#3        | 8.59M/63.6M [00:06&lt;00:37, 1.56MB/s]
 14%|#3        | 8.76M/63.6M [00:06&lt;00:36, 1.56MB/s]
 14%|#4        | 8.91M/63.6M [00:06&lt;00:37, 1.55MB/s]
 14%|#4        | 9.08M/63.6M [00:06&lt;00:36, 1.56MB/s]
 15%|#4        | 9.24M/63.6M [00:06&lt;00:36, 1.56MB/s]
 15%|#4        | 9.41M/63.6M [00:07&lt;00:36, 1.56MB/s]
 15%|#5        | 9.57M/63.6M [00:07&lt;00:36, 1.57MB/s]
 15%|#5        | 9.74M/63.6M [00:07&lt;00:35, 1.59MB/s]
 16%|#5        | 9.91M/63.6M [00:07&lt;00:35, 1.59MB/s]
 16%|#5        | 10.1M/63.6M [00:07&lt;00:35, 1.58MB/s]
 16%|#6        | 10.2M/63.6M [00:07&lt;00:35, 1.58MB/s]
 16%|#6        | 10.4M/63.6M [00:07&lt;00:34, 1.60MB/s]
 17%|#6        | 10.6M/63.6M [00:07&lt;00:34, 1.64MB/s]
 17%|#6        | 10.8M/63.6M [00:07&lt;00:34, 1.62MB/s]
 17%|#7        | 10.9M/63.6M [00:07&lt;00:33, 1.67MB/s]
 17%|#7        | 11.1M/63.6M [00:08&lt;00:33, 1.64MB/s]
 18%|#7        | 11.3M/63.6M [00:08&lt;00:32, 1.66MB/s]
 18%|#8        | 11.5M/63.6M [00:08&lt;00:32, 1.70MB/s]
 18%|#8        | 11.6M/63.6M [00:08&lt;00:32, 1.67MB/s]
 19%|#8        | 11.8M/63.6M [00:08&lt;00:32, 1.68MB/s]
 19%|#8        | 12.0M/63.6M [00:08&lt;00:30, 1.76MB/s]
 19%|#9        | 12.2M/63.6M [00:08&lt;00:30, 1.75MB/s]
 19%|#9        | 12.4M/63.6M [00:08&lt;00:30, 1.77MB/s]
 20%|#9        | 12.6M/63.6M [00:08&lt;00:29, 1.80MB/s]
 20%|##        | 12.8M/63.6M [00:09&lt;00:28, 1.84MB/s]
 20%|##        | 13.0M/63.6M [00:09&lt;00:28, 1.85MB/s]
 21%|##        | 13.2M/63.6M [00:09&lt;00:28, 1.88MB/s]
 21%|##1       | 13.4M/63.6M [00:09&lt;00:28, 1.88MB/s]
 21%|##1       | 13.6M/63.6M [00:09&lt;00:27, 1.94MB/s]
 22%|##1       | 13.8M/63.6M [00:09&lt;00:26, 1.96MB/s]
 22%|##2       | 14.0M/63.6M [00:09&lt;00:25, 2.00MB/s]
 22%|##2       | 14.2M/63.6M [00:09&lt;00:25, 2.04MB/s]
 23%|##2       | 14.5M/63.6M [00:09&lt;00:25, 2.03MB/s]
 23%|##3       | 14.7M/63.6M [00:10&lt;00:24, 2.10MB/s]
 23%|##3       | 14.9M/63.6M [00:10&lt;00:23, 2.14MB/s]
 24%|##3       | 15.2M/63.6M [00:10&lt;00:22, 2.21MB/s]
 24%|##4       | 15.4M/63.6M [00:10&lt;00:22, 2.20MB/s]
 25%|##4       | 15.6M/63.6M [00:10&lt;00:22, 2.26MB/s]
 25%|##5       | 15.9M/63.6M [00:10&lt;00:21, 2.34MB/s]
 25%|##5       | 16.2M/63.6M [00:10&lt;00:21, 2.36MB/s]
 26%|##5       | 16.4M/63.6M [00:10&lt;00:20, 2.42MB/s]
 26%|##6       | 16.7M/63.6M [00:10&lt;00:19, 2.50MB/s]
 27%|##6       | 17.0M/63.6M [00:11&lt;00:19, 2.56MB/s]
 27%|##7       | 17.3M/63.6M [00:11&lt;00:18, 2.60MB/s]
 28%|##7       | 17.6M/63.6M [00:11&lt;00:17, 2.69MB/s]
 28%|##8       | 17.9M/63.6M [00:11&lt;00:17, 2.70MB/s]
 29%|##8       | 18.2M/63.6M [00:11&lt;00:17, 2.78MB/s]
 29%|##9       | 18.5M/63.6M [00:11&lt;00:16, 2.87MB/s]
 30%|##9       | 18.8M/63.6M [00:11&lt;00:16, 2.91MB/s]
 30%|###       | 19.1M/63.6M [00:11&lt;00:15, 2.98MB/s]
 31%|###       | 19.5M/63.6M [00:11&lt;00:15, 3.05MB/s]
 31%|###1      | 19.8M/63.6M [00:12&lt;00:14, 3.15MB/s]
 32%|###1      | 20.2M/63.6M [00:12&lt;00:14, 3.23MB/s]
 32%|###2      | 20.5M/63.6M [00:12&lt;00:13, 3.31MB/s]
 33%|###2      | 20.9M/63.6M [00:12&lt;00:13, 3.38MB/s]
 33%|###3      | 21.3M/63.6M [00:12&lt;00:12, 3.47MB/s]
 34%|###4      | 21.7M/63.6M [00:12&lt;00:12, 3.57MB/s]
 35%|###4      | 22.1M/63.6M [00:12&lt;00:11, 3.65MB/s]
 35%|###5      | 22.5M/63.6M [00:12&lt;00:11, 3.75MB/s]
 36%|###6      | 22.9M/63.6M [00:12&lt;00:11, 3.85MB/s]
 37%|###6      | 23.4M/63.6M [00:13&lt;00:10, 3.94MB/s]
 37%|###7      | 23.8M/63.6M [00:13&lt;00:10, 4.06MB/s]
 38%|###8      | 24.3M/63.6M [00:13&lt;00:09, 4.16MB/s]
 39%|###8      | 24.8M/63.6M [00:13&lt;00:09, 4.27MB/s]
 40%|###9      | 25.2M/63.6M [00:13&lt;00:09, 4.36MB/s]
 40%|####      | 25.7M/63.6M [00:13&lt;00:08, 4.52MB/s]
 41%|####1     | 26.2M/63.6M [00:13&lt;00:08, 4.61MB/s]
 42%|####2     | 26.8M/63.6M [00:13&lt;00:08, 4.72MB/s]
 43%|####2     | 27.3M/63.6M [00:13&lt;00:07, 4.87MB/s]
 44%|####3     | 27.9M/63.6M [00:14&lt;00:07, 4.98MB/s]
 45%|####4     | 28.4M/63.6M [00:14&lt;00:07, 5.12MB/s]
 46%|####5     | 29.0M/63.6M [00:14&lt;00:06, 5.28MB/s]
 47%|####6     | 29.6M/63.6M [00:14&lt;00:06, 5.40MB/s]
 47%|####7     | 30.2M/63.6M [00:14&lt;00:06, 5.57MB/s]
 48%|####8     | 30.9M/63.6M [00:14&lt;00:06, 5.70MB/s]
 50%|####9     | 31.5M/63.6M [00:14&lt;00:05, 5.83MB/s]
 51%|#####     | 32.2M/63.6M [00:14&lt;00:05, 6.01MB/s]
 52%|#####1    | 32.8M/63.6M [00:14&lt;00:05, 6.15MB/s]
 53%|#####2    | 33.5M/63.6M [00:14&lt;00:04, 6.31MB/s]
 54%|#####3    | 34.2M/63.6M [00:15&lt;00:04, 6.47MB/s]
 55%|#####4    | 35.0M/63.6M [00:15&lt;00:04, 6.63MB/s]
 56%|#####6    | 35.7M/63.6M [00:15&lt;00:04, 6.83MB/s]
 57%|#####7    | 36.5M/63.6M [00:15&lt;00:04, 6.98MB/s]
 59%|#####8    | 37.3M/63.6M [00:15&lt;00:03, 7.15MB/s]
 60%|#####9    | 38.1M/63.6M [00:15&lt;00:03, 7.32MB/s]
 61%|######1   | 38.9M/63.6M [00:15&lt;00:03, 7.56MB/s]
 63%|######2   | 39.8M/63.6M [00:15&lt;00:03, 7.70MB/s]
 64%|######3   | 40.7M/63.6M [00:15&lt;00:03, 7.91MB/s]
 65%|######5   | 41.6M/63.6M [00:16&lt;00:02, 8.10MB/s]
 67%|######6   | 42.5M/63.6M [00:16&lt;00:02, 8.32MB/s]
 68%|######8   | 43.4M/63.6M [00:16&lt;00:02, 8.52MB/s]
 70%|######9   | 44.4M/63.6M [00:16&lt;00:02, 8.71MB/s]
 71%|#######1  | 45.4M/63.6M [00:16&lt;00:02, 8.94MB/s]
 73%|#######2  | 46.4M/63.6M [00:16&lt;00:01, 9.16MB/s]
 74%|#######4  | 47.4M/63.6M [00:16&lt;00:01, 9.36MB/s]
 76%|#######6  | 48.5M/63.6M [00:16&lt;00:01, 9.59MB/s]
 78%|#######7  | 49.5M/63.6M [00:16&lt;00:01, 9.87MB/s]
 80%|#######9  | 50.7M/63.6M [00:17&lt;00:01, 10.2MB/s]
 81%|########1 | 51.8M/63.6M [00:17&lt;00:01, 10.3MB/s]
 83%|########3 | 53.0M/63.6M [00:17&lt;00:01, 10.5MB/s]
 85%|########5 | 54.2M/63.6M [00:17&lt;00:00, 10.8MB/s]
 87%|########7 | 55.4M/63.6M [00:17&lt;00:00, 11.0MB/s]
 89%|########8 | 56.6M/63.6M [00:17&lt;00:00, 11.3MB/s]
 91%|######### | 57.9M/63.6M [00:17&lt;00:00, 11.6MB/s]
 93%|#########3| 59.2M/63.6M [00:17&lt;00:00, 12.0MB/s]
 95%|#########5| 60.5M/63.6M [00:17&lt;00:00, 12.0MB/s]
 97%|#########7| 61.9M/63.6M [00:18&lt;00:00, 12.3MB/s]
 99%|#########9| 63.3M/63.6M [00:18&lt;00:00, 12.6MB/s]
100%|##########| 63.6M/63.6M [00:18&lt;00:00, 3.67MB/s]
tensor([[54, 20, 65, 69, 11, 92, 44, 65, 38,  2, 11, 81, 40, 64, 79, 81, 11, 81,
         20, 11, 79, 77, 59, 37,  2]])
tensor([25], dtype=torch.int32)
</pre></div>
</div>
<p>Notice that the encoded values are different from the example of
character-based encoding.</p>
<p>The intermediate representation looks like the following.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">([</span><a class="sphx-glr-backref-module-torchaudio-pipelines-Tacotron2TTSBundle sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.TextProcessor.tokens" title="torchaudio.pipelines.Tacotron2TTSBundle.TextProcessor.tokens"><span class="n">processor</span><span class="o">.</span><span class="n">tokens</span></a><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]]])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>['HH', 'AH', 'L', 'OW', ' ', 'W', 'ER', 'L', 'D', '!', ' ', 'T', 'EH', 'K', 'S', 'T', ' ', 'T', 'AH', ' ', 'S', 'P', 'IY', 'CH', '!']
</pre></div>
</div>
</div>
</div>
<div class="section" id="spectrogram-generation">
<h2>Spectrogram Generation<a class="headerlink" href="#spectrogram-generation" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Tacotron2</span></code> is the model we use to generate spectrogram from the
encoded text. For the detail of the model, please refer to <a class="reference external" href="https://arxiv.org/abs/1712.05884">the
paper</a>.</p>
<p>It is easy to instantiate a Tacotron2 model with pretrained weight,
however, note that the input to Tacotron2 models are processed by the
matching text processor.</p>
<p><code class="docutils literal notranslate"><span class="pre">torchaudio</span></code> bundles the matching models and processors together so
that it is easy to create the pipeline.</p>
<p>(For the available bundles, and its usage, please refer to <a class="reference external" href="https://pytorch.org/audio/stable/pipelines.html#tacotron2-text-to-speech">the
documentation</a>.)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bundle</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-data" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH" title="torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH"><span class="n">torchaudio</span><span class="o">.</span><span class="n">pipelines</span><span class="o">.</span><span class="n">TACOTRON2_WAVERNN_PHONE_LJSPEECH</span></a>
<span class="n">processor</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor" title="torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor"><span class="n">bundle</span><span class="o">.</span><span class="n">get_text_processor</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torchaudio-models sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/audio/stable/models.html#torchaudio.models.Tacotron2" title="torchaudio.models.Tacotron2"><span class="n">tacotron2</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_tacotron2" title="torchaudio.pipelines.Tacotron2TTSBundle.get_tacotron2"><span class="n">bundle</span><span class="o">.</span><span class="n">get_tacotron2</span></a><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello world! Text to speech!"</span>

<span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode" title="torch.inference_mode"><span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span></a><span class="p">():</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">_</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">_</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-models sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/models.html#torchaudio.models.Tacotron2.infer" title="torchaudio.models.Tacotron2.infer"><span class="n">tacotron2</span><span class="o">.</span><span class="n">infer</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
</pre></div>
</div>
<img alt="text to speech with torchaudio" class="sphx-glr-single-img" src="../_images/sphx_glr_text_to_speech_with_torchaudio_001.png" srcset="../_images/sphx_glr_text_to_speech_with_torchaudio_001.png"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading: "https://download.pytorch.org/torchaudio/models/tacotron2_english_phonemes_1500_epochs_wavernn_ljspeech.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/tacotron2_english_phonemes_1500_epochs_wavernn_ljspeech.pth

  0%|          | 0.00/107M [00:00&lt;?, ?B/s]
  9%|9         | 9.85M/107M [00:00&lt;00:00, 103MB/s]
 23%|##2       | 24.3M/107M [00:00&lt;00:00, 131MB/s]
 39%|###9      | 42.1M/107M [00:00&lt;00:00, 157MB/s]
 58%|#####8    | 62.6M/107M [00:00&lt;00:00, 180MB/s]
 77%|#######6  | 82.8M/107M [00:00&lt;00:00, 191MB/s]
 97%|#########6| 104M/107M [00:00&lt;00:00, 202MB/s]
100%|##########| 107M/107M [00:00&lt;00:00, 163MB/s]

&lt;matplotlib.image.AxesImage object at 0x7f4b083943d0&gt;
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">Tacotron2.infer</span></code> method perfoms multinomial sampling,
therefor, the process of generating the spectrogram incurs randomness.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">_</span></a> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
  <span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode" title="torch.inference_mode"><span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span></a><span class="p">():</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec_lengths</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">_</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-models sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/models.html#torchaudio.models.Tacotron2.infer" title="torchaudio.models.Tacotron2.infer"><span class="n">tacotron2</span><span class="o">.</span><span class="n">infer</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="text to speech with torchaudio" class="sphx-glr-single-img" src="../_images/sphx_glr_text_to_speech_with_torchaudio_002.png" srcset="../_images/sphx_glr_text_to_speech_with_torchaudio_002.png"/></div>
<div class="section" id="waveform-generation">
<h2>Waveform Generation<a class="headerlink" href="#waveform-generation" title="Permalink to this heading">¶</a></h2>
<p>Once the spectrogram is generated, the last process is to recover the
waveform from the spectrogram.</p>
<p><code class="docutils literal notranslate"><span class="pre">torchaudio</span></code> provides vocoders based on <code class="docutils literal notranslate"><span class="pre">GriffinLim</span></code> and
<code class="docutils literal notranslate"><span class="pre">WaveRNN</span></code>.</p>
<div class="section" id="wavernn">
<h3>WaveRNN<a class="headerlink" href="#wavernn" title="Permalink to this heading">¶</a></h3>
<p>Continuing from the previous section, we can instantiate the matching
WaveRNN model from the same bundle.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bundle</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-data" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH" title="torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH"><span class="n">torchaudio</span><span class="o">.</span><span class="n">pipelines</span><span class="o">.</span><span class="n">TACOTRON2_WAVERNN_PHONE_LJSPEECH</span></a>

<span class="n">processor</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor" title="torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor"><span class="n">bundle</span><span class="o">.</span><span class="n">get_text_processor</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torchaudio-models sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/audio/stable/models.html#torchaudio.models.Tacotron2" title="torchaudio.models.Tacotron2"><span class="n">tacotron2</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_tacotron2" title="torchaudio.pipelines.Tacotron2TTSBundle.get_tacotron2"><span class="n">bundle</span><span class="o">.</span><span class="n">get_tacotron2</span></a><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">vocoder</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_vocoder" title="torchaudio.pipelines.Tacotron2TTSBundle.get_vocoder"><span class="n">bundle</span><span class="o">.</span><span class="n">get_vocoder</span></a><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello world! Text to speech!"</span>

<span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode" title="torch.inference_mode"><span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span></a><span class="p">():</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec_lengths</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">_</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-models sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/models.html#torchaudio.models.Tacotron2.infer" title="torchaudio.models.Tacotron2.infer"><span class="n">tacotron2</span><span class="o">.</span><span class="n">infer</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">waveforms</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <span class="n">vocoder</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec_lengths</span></a><span class="p">)</span>

<a class="sphx-glr-backref-module-torchaudio sphx-glr-backref-type-py-function" href="https://pytorch.org/audio/stable/torchaudio.html#torchaudio.save" title="torchaudio.save"><span class="n">torchaudio</span><span class="o">.</span><span class="n">save</span></a><span class="p">(</span><span class="s2">"output_wavernn.wav"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">waveforms</span></a><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">sample_rate</span><span class="o">=</span><a class="sphx-glr-backref-module-torchaudio-pipelines-Tacotron2TTSBundle sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.Vocoder.sample_rate" title="torchaudio.pipelines.Tacotron2TTSBundle.Vocoder.sample_rate"><span class="n">vocoder</span><span class="o">.</span><span class="n">sample_rate</span></a><span class="p">)</span>
<span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Audio</span><span class="p">(</span><span class="s2">"output_wavernn.wav"</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading: "https://download.pytorch.org/torchaudio/models/wavernn_10k_epochs_8bits_ljspeech.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/wavernn_10k_epochs_8bits_ljspeech.pth

  0%|          | 0.00/16.7M [00:00&lt;?, ?B/s]
  0%|          | 40.0k/16.7M [00:00&lt;01:04, 272kB/s]
  0%|          | 80.0k/16.7M [00:00&lt;00:53, 322kB/s]
  1%|1         | 208k/16.7M [00:00&lt;00:27, 637kB/s]
  3%|2         | 480k/16.7M [00:00&lt;00:13, 1.22MB/s]
  5%|5         | 912k/16.7M [00:00&lt;00:08, 2.01MB/s]
  8%|7         | 1.30M/16.7M [00:00&lt;00:06, 2.64MB/s]
 10%|9         | 1.64M/16.7M [00:00&lt;00:05, 2.90MB/s]
 12%|#2        | 2.00M/16.7M [00:00&lt;00:04, 3.13MB/s]
 14%|#4        | 2.38M/16.7M [00:01&lt;00:04, 3.35MB/s]
 17%|#6        | 2.83M/16.7M [00:01&lt;00:04, 3.58MB/s]
 20%|#9        | 3.32M/16.7M [00:01&lt;00:03, 4.01MB/s]
 22%|##2       | 3.74M/16.7M [00:01&lt;00:03, 4.09MB/s]
 25%|##5       | 4.19M/16.7M [00:01&lt;00:03, 4.21MB/s]
 28%|##7       | 4.64M/16.7M [00:01&lt;00:02, 4.34MB/s]
 31%|###1      | 5.19M/16.7M [00:01&lt;00:02, 4.55MB/s]
 35%|###4      | 5.78M/16.7M [00:01&lt;00:02, 5.02MB/s]
 38%|###7      | 6.28M/16.7M [00:01&lt;00:02, 5.07MB/s]
 41%|####      | 6.80M/16.7M [00:02&lt;00:02, 5.16MB/s]
 44%|####4     | 7.35M/16.7M [00:02&lt;00:01, 5.31MB/s]
 48%|####8     | 8.05M/16.7M [00:02&lt;00:01, 5.56MB/s]
 53%|#####2    | 8.77M/16.7M [00:02&lt;00:01, 6.10MB/s]
 56%|#####6    | 9.38M/16.7M [00:02&lt;00:01, 6.18MB/s]
 60%|######    | 10.0M/16.7M [00:02&lt;00:01, 6.30MB/s]
 64%|######3   | 10.7M/16.7M [00:02&lt;00:00, 6.44MB/s]
 69%|######8   | 11.5M/16.7M [00:02&lt;00:00, 7.11MB/s]
 73%|#######3  | 12.2M/16.7M [00:02&lt;00:00, 7.18MB/s]
 78%|#######7  | 12.9M/16.7M [00:02&lt;00:00, 7.32MB/s]
 82%|########2 | 13.7M/16.7M [00:03&lt;00:00, 7.49MB/s]
 88%|########7 | 14.6M/16.7M [00:03&lt;00:00, 7.82MB/s]
 94%|#########3| 15.6M/16.7M [00:03&lt;00:00, 8.55MB/s]
 99%|#########8| 16.5M/16.7M [00:03&lt;00:00, 8.65MB/s]
100%|##########| 16.7M/16.7M [00:03&lt;00:00, 5.19MB/s]
&lt;IPython.lib.display.Audio object&gt;
</pre></div>
</div>
</div>
<div class="section" id="griffin-lim">
<h3>Griffin-Lim<a class="headerlink" href="#griffin-lim" title="Permalink to this heading">¶</a></h3>
<p>Using the Griffin-Lim vocoder is same as WaveRNN. You can instantiate
the vocode object with <code class="docutils literal notranslate"><span class="pre">get_vocoder</span></code> method and pass the spectrogram.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bundle</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-data" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH" title="torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH"><span class="n">torchaudio</span><span class="o">.</span><span class="n">pipelines</span><span class="o">.</span><span class="n">TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH</span></a>

<span class="n">processor</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor" title="torchaudio.pipelines.Tacotron2TTSBundle.get_text_processor"><span class="n">bundle</span><span class="o">.</span><span class="n">get_text_processor</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torchaudio-models sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/audio/stable/models.html#torchaudio.models.Tacotron2" title="torchaudio.models.Tacotron2"><span class="n">tacotron2</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_tacotron2" title="torchaudio.pipelines.Tacotron2TTSBundle.get_tacotron2"><span class="n">bundle</span><span class="o">.</span><span class="n">get_tacotron2</span></a><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">vocoder</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-pipelines sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.get_vocoder" title="torchaudio.pipelines.Tacotron2TTSBundle.get_vocoder"><span class="n">bundle</span><span class="o">.</span><span class="n">get_vocoder</span></a><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode" title="torch.inference_mode"><span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span></a><span class="p">():</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec_lengths</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">_</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchaudio-models sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/models.html#torchaudio.models.Tacotron2.infer" title="torchaudio.models.Tacotron2.infer"><span class="n">tacotron2</span><span class="o">.</span><span class="n">infer</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">processed</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">waveforms</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <span class="n">vocoder</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec_lengths</span></a><span class="p">)</span>

<a class="sphx-glr-backref-module-torchaudio sphx-glr-backref-type-py-function" href="https://pytorch.org/audio/stable/torchaudio.html#torchaudio.save" title="torchaudio.save"><span class="n">torchaudio</span><span class="o">.</span><span class="n">save</span></a><span class="p">(</span><span class="s2">"output_griffinlim.wav"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">waveforms</span></a><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">sample_rate</span><span class="o">=</span><a class="sphx-glr-backref-module-torchaudio-pipelines-Tacotron2TTSBundle sphx-glr-backref-type-py-method" href="https://pytorch.org/audio/stable/pipelines.html#torchaudio.pipelines.Tacotron2TTSBundle.Vocoder.sample_rate" title="torchaudio.pipelines.Tacotron2TTSBundle.Vocoder.sample_rate"><span class="n">vocoder</span><span class="o">.</span><span class="n">sample_rate</span></a><span class="p">)</span>
<span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Audio</span><span class="p">(</span><span class="s2">"output_griffinlim.wav"</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading: "https://download.pytorch.org/torchaudio/models/tacotron2_english_phonemes_1500_epochs_ljspeech.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/tacotron2_english_phonemes_1500_epochs_ljspeech.pth

  0%|          | 0.00/107M [00:00&lt;?, ?B/s]
  7%|6         | 7.42M/107M [00:00&lt;00:01, 77.8MB/s]
 16%|#5        | 16.8M/107M [00:00&lt;00:01, 89.8MB/s]
 24%|##4       | 26.1M/107M [00:00&lt;00:00, 93.6MB/s]
 33%|###3      | 35.7M/107M [00:00&lt;00:00, 96.0MB/s]
 42%|####2     | 45.3M/107M [00:00&lt;00:00, 97.8MB/s]
 51%|#####     | 54.7M/107M [00:00&lt;00:00, 98.1MB/s]
 60%|#####9    | 64.1M/107M [00:00&lt;00:00, 96.5MB/s]
 68%|######8   | 73.3M/107M [00:00&lt;00:00, 95.7MB/s]
 77%|#######7  | 82.9M/107M [00:00&lt;00:00, 97.4MB/s]
 86%|########6 | 92.6M/107M [00:01&lt;00:00, 98.7MB/s]
 95%|#########4| 102M/107M [00:01&lt;00:00, 98.1MB/s]
100%|##########| 107M/107M [00:01&lt;00:00, 96.5MB/s]
&lt;IPython.lib.display.Audio object&gt;
</pre></div>
</div>
</div>
<div class="section" id="waveglow">
<h3>Waveglow<a class="headerlink" href="#waveglow" title="Permalink to this heading">¶</a></h3>
<p>Waveglow is a vocoder published by Nvidia. The pretrained weight is
publishe on Torch Hub. One can instantiate the model using <code class="docutils literal notranslate"><span class="pre">torch.hub</span></code>
module.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">waveglow</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-hub sphx-glr-backref-type-py-function" href="https://pytorch.org/docs/stable/hub.html#torch.hub.load" title="torch.hub.load"><span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><span class="s1">'NVIDIA/DeepLearningExamples:torchhub'</span><span class="p">,</span> <span class="s1">'nvidia_waveglow'</span><span class="p">,</span> <span class="n">model_math</span><span class="o">=</span><span class="s1">'fp32'</span><span class="p">)</span>
<span class="n">waveglow</span> <span class="o">=</span> <span class="n">waveglow</span><span class="o">.</span><span class="n">remove_weightnorm</span><span class="p">(</span><span class="n">waveglow</span><span class="p">)</span>
<span class="n">waveglow</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to" title="torch.nn.Module.to"><span class="n">waveglow</span><span class="o">.</span><span class="n">to</span></a><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval"><span class="n">waveglow</span><span class="o">.</span><span class="n">eval</span></a><span class="p">()</span>

<span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
  <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">waveforms</span></a> <span class="o">=</span> <span class="n">waveglow</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">spec</span></a><span class="p">)</span>

<a class="sphx-glr-backref-module-torchaudio sphx-glr-backref-type-py-function" href="https://pytorch.org/audio/stable/torchaudio.html#torchaudio.save" title="torchaudio.save"><span class="n">torchaudio</span><span class="o">.</span><span class="n">save</span></a><span class="p">(</span><span class="s2">"output_waveglow.wav"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">waveforms</span></a><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">22050</span><span class="p">)</span>
<span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Audio</span><span class="p">(</span><span class="s2">"output_waveglow.wav"</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.7/site-packages/torch/hub.py:267: UserWarning:

You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour

Downloading: "https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub" to /var/lib/jenkins/.cache/torch/hub/torchhub.zip
/var/lib/jenkins/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning:

pytorch_quantization module not found, quantization will not be available

/var/lib/jenkins/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning:

pytorch_quantization module not found, quantization will not be available

Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427
&lt;IPython.lib.display.Audio object&gt;
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 3 minutes  2.226 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-text-to-speech-with-torchaudio-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b92c8b4105929474fc77e408a3f1243a/text_to_speech_with_torchaudio.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">text_to_speech_with_torchaudio.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/c5bc4fe7be48aec5b1795649a970e947/text_to_speech_with_torchaudio.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">text_to_speech_with_torchaudio.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</div>
</article>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="forced_alignment_with_torchaudio_tutorial.html" rel="next" title="Forced Alignment with Wav2Vec2">Next <img class="next-page" src="../_static/images/chevron-right-orange.svg"/></a>
<a accesskey="p" class="btn btn-neutral" href="speech_command_classification_with_torchaudio_tutorial.html" rel="prev" title="Speech Command Classification with torchaudio"><img class="previous-page" src="../_static/images/chevron-right-orange.svg"/> Previous</a>
</div>
<hr class="rating-hr hr-top"/>
<div class="rating-container">
<div class="rating-prompt">Rate this Tutorial</div>
<div class="stars-outer">
<i class="far fa-star" data-behavior="tutorial-rating" data-count="1" title="1 Star"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="2" title="2 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="3" title="3 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="4" title="4 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="5" title="5 Stars"></i>
</div>
</div>
<hr class="rating-hr hr-bottom"/>
<div role="contentinfo">
<p>
        © Copyright 2022, PyTorch.

    </p>
</div>
<div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
</footer>
</div>
<script>
if((window.location.href.indexOf("/prototype/")!= -1) && (window.location.href.indexOf("/prototype/prototype_index")< 1))
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-flask" aria-hidden="true">&nbsp</i> This tutorial describes a prototype feature. Prototype features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  } 
</script>
</div>
<div class="pytorch-content-right" id="pytorch-content-right">
<div class="pytorch-right-menu" id="pytorch-right-menu">
<div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
<ul>
<li><a class="reference internal" href="#">Text-to-speech with torchaudio</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#preparation">Preparation</a></li>
<li><a class="reference internal" href="#text-processing">Text Processing</a><ul>
<li><a class="reference internal" href="#character-based-encoding">Character-based encoding</a></li>
<li><a class="reference internal" href="#phoneme-based-encoding">Phoneme-based encoding</a></li>
</ul>
</li>
<li><a class="reference internal" href="#spectrogram-generation">Spectrogram Generation</a></li>
<li><a class="reference internal" href="#waveform-generation">Waveform Generation</a><ul>
<li><a class="reference internal" href="#wavernn">WaveRNN</a></li>
<li><a class="reference internal" href="#griffin-lim">Griffin-Lim</a></li>
<li><a class="reference internal" href="#waveglow">Waveglow</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</section>
</div>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/katex.min.js"></script>
<script src="../_static/auto-render.min.js"></script>
<script src="../_static/katex_autorenderer.js"></script>
<script src="../_static/design-tabs.js"></script>
<script src="../_static/js/vendor/popper.min.js" type="text/javascript"></script>
<script src="../_static/js/vendor/bootstrap.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<script>

  
//add microsoft link

if(window.location.href.indexOf("/beginner/basics/")!= -1)
{
  var url="https://docs.microsoft.com/learn/paths/pytorch-fundamentals/?wt.mc_id=aiml-7486-cxa";
  switch(window.location.pathname.split("/").pop().replace('.html',''))
  {
    case"quickstart_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/9-quickstart?WT.mc_id=aiml-7486-cxa";
      break;
    case"tensorqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/2-tensors?WT.mc_id=aiml-7486-cxa";
      break;
    case"data_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/3-data?WT.mc_id=aiml-7486-cxa";
      break;
    case"transforms_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/4-transforms?WT.mc_id=aiml-7486-cxa";
      break;
    case"buildmodel_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/5-model?WT.mc_id=aiml-7486-cxa";
      break;
    case"autogradqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/6-autograd?WT.mc_id=aiml-7486-cxa";
      break;
    case"optimization_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/7-optimization?WT.mc_id=aiml-7486-cxa";
      break;
    case"saveloadrun_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/8-inference?WT.mc_id=aiml-7486-cxa";
    }
    
    $(".pytorch-call-to-action-links").children().first().before("<a href="+url+' data-behavior="call-to-action-event" data-response="Run in Microsoft Learn" target="_blank"><div id="microsoft-learn-link" style="padding-bottom: 0.625rem;border-bottom: 1px solid #f3f4f7;padding-right: 2.5rem;display: -webkit-box;  display: -ms-flexbox; isplay: flex; -webkit-box-align: center;-ms-flex-align: center;align-items: center;"><img class="call-to-action-img" src="../../_static/images/microsoft-logo.svg"/><div class="call-to-action-desktop-view">Run in Microsoft Learn</div><div class="call-to-action-mobile-view">Learn</div></div></a>')
  }

  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    fbq('trackCustom', "Download", {
      tutorialTitle: $('h1:first').text(),
      downloadLink: this.href,
      tutorialLink: window.location.href,
      downloadTitle: $(this).attr("data-response")
    });
    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='tutorial-rating']").on('click', function(){
    fbq('trackCustom', "Tutorial Rating", {
      tutorialLink: window.location.href,
      tutorialTitle: $('h1:first').text(),
      rating: $(this).attr("data-count")
    });

    gtag('event', 'click', {
      'event_category': 'Tutorial Rating',
      'event_label': $("h1").first().text(),
      'value': $(this).attr("data-count")
    });
   });

   if (location.pathname == "/") {
     $(".rating-container").hide();
     $(".hr-bottom").hide();
   }


</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView
  &amp;noscript=1" width="1"/>
</noscript>
<script type="text/javascript">
  var collapsedSections = ['PyTorch Recipes', 'Learning PyTorch', 'Image and Video', 'Audio', 'Text', 'Reinforcement Learning', 'Deploying PyTorch Models in Production', 'Code Transforms with FX', 'Frontend APIs', 'Extending PyTorch', 'Model Optimization', 'Parallel and Distributed Training', 'Mobile'];
</script>
<img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1"/>
<!-- Begin Footer -->
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4 text-center">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4 text-center">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4 text-center">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="footer-logo-wrapper">
<a class="footer-logo" href="https://pytorch.org/"></a>
</div>
<div class="footer-links-wrapper">
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
<li><a href="https://pytorch.org/get-started">Get Started</a></li>
<li><a href="https://pytorch.org/features">Features</a></li>
<li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
<li><a href="https://pytorch.org/blog/">Blog</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
<li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
<li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
<li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
<li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title">Stay up to date</li>
<li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title">PyTorch Podcasts</li>
<li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
<li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
<li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
<li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
</ul>
</div>
</div>
<div class="privacy-policy">
<ul>
<li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
<li class="privacy-policy-links">|</li>
<li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
</ul>
</div>
<div class="copyright">
<p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/images/pytorch-x.svg">
</img></div>
</div>
<!-- End Footer -->
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="container">
<div class="mobile-main-menu-header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#"></a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/ecosystem">Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/mobile">Mobile</a>
</li>
<li>
<a href="https://pytorch.org/hub">PyTorch Hub</a>
</li>
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li class="active">
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li class="resources-mobile-menu-title">
            Docs
          </li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
</li>
<li>
<a href="https://pytorch.org/text/stable/index.html">torchtext</a>
</li>
<li>
<a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
</li>
<li>
<a href="https://pytorch.org/serve/">TorchServe</a>
</li>
<li>
<a href="https://pytorch.org/torchx/">TorchX</a>
</li>
<li>
<a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
            Resources
          </li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/features">About</a>
</li>
<li>
<a href="https://pytorch.org/hub">Models (Beta)</a>
</li>
<li>
<a href="https://pytorch.org/#community-module">Community</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
</ul>
<li>
<a href="https://github.com/pytorch/pytorch">Github</a>
</li>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<script src="../_static/js/vendor/anchor.min.js" type="text/javascript"></script>
<script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>